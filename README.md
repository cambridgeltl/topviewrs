<br />
<p align="center">
  <h1 align="center">👀 TopViewRS: Vision-Language Models as Top-View Spatial Reasoners</h1>
  <h3 align="center">A novel evaluation benchmark for spatial reasoning of vision-language models.</h3>
  
  <p align="center">  
    <a href="https://arxiv.org/abs/2406.02537">📄 [Arxiv]</a>
    ·
    <a href="https://topviewrs.github.io/">🕸️ [Project Page]</a>
    ·
    <a href="dataset_placeholder">🤗 [Data(Coming Soon)]</a>
    
  </p>
</p>

## Key takeaways

> * Define top-view spatial reasoning task for VLMs via 4 carefully designed tasks of increasing complexity, also encompassing 9 distinct fine-grained sub-tasks with a structured design of the questions focusing on different model abilities.
> * Collect **TopViewRS Dataset** (**Top-View** **R**easoning in **S**pace), comprising 11,384 multiple-choice questions with either _photo-realistic_ or _semantic_ top-view maps of real-world scenarios
> * Investigate 10 VLMs from different model families and sizes, highlighting the _performance gap_ compared to human annotators.

![sicl](figs/main_fig.png)

## Dataset
Coming soon. Stay tuned :)

## Code
Coming soon. 

## Citation
If you find TopViewRS useful:
```bibtex
@misc{li2024topviewrs,
      title={TopViewRS: Vision-Language Models as Top-View Spatial Reasoners}, 
      author={Chengzu Li and Caiqi Zhang and Han Zhou and Nigel Collier and Anna Korhonen and Ivan Vulić},
      year={2024},
      eprint={2406.02537},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```